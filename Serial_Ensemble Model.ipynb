{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agm2D7MH16Mv"
      },
      "outputs": [],
      "source": [
        "pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcCXqtayzfTm"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import  svm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import catboost\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn import tree\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nk4sVJvGv0e"
      },
      "outputs": [],
      "source": [
        "dfa = pd.read_csv(r'...') # tertiary dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6acAoRImxc0P"
      },
      "outputs": [],
      "source": [
        "df = dfa.drop(columns = ['SubjectID', 'VideoID','predefinedlabel'])\n",
        "#df = df.drop(columns = 'SubjectID')\n",
        "df.dropna()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHm-d0tdG1ro"
      },
      "source": [
        "Creating sub datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OH6j8ImMG6os"
      },
      "outputs": [],
      "source": [
        "df1 = df.sample(frac =0.8)\n",
        "df2 = df.sample(frac =0.8)\n",
        "df3 = df.sample(frac =0.8)\n",
        "df4 = df.sample(frac =0.8)\n",
        "df5 = df.sample(frac =0.8)\n",
        "df6 = df.sample(frac =0.8)\n",
        "df7 = df.sample(frac =0.8)\n",
        "df1.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_4AG78pISvG"
      },
      "source": [
        "Splitting subset data into train validate and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4O-vumJ1yiF"
      },
      "outputs": [],
      "source": [
        "train_df1, test_df1 = train_test_split(df1, test_size = 0.3, random_state = 113)\n",
        "train_df2, test_df2 = train_test_split(df2, test_size = 0.3, random_state = 113)\n",
        "train_df3, test_df3 = train_test_split(df3, test_size = 0.3, random_state = 113)\n",
        "train_df4, test_df4 = train_test_split(df4, test_size = 0.3, random_state = 113)\n",
        "train_df5, test_df5 = train_test_split(df5, test_size = 0.3, random_state = 113)\n",
        "train_df7, test_df7 = train_test_split(df7, test_size = 0.3, random_state = 113)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIlt36VmI2EI"
      },
      "outputs": [],
      "source": [
        "ic1 = df.columns.tolist() #features\n",
        "ic2 = df2.columns.tolist() #features\n",
        "ic3 = df3.columns.tolist() #features\n",
        "ic4 = df4.columns.tolist() #features\n",
        "ic5 = df5.columns.tolist() #features\n",
        "ic6 = df6.columns.tolist() #features\n",
        "ic7 = df7.columns.tolist() #features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDr1RQf0KfD_"
      },
      "outputs": [],
      "source": [
        "ic1.remove('user-definedlabeln')\n",
        "oc1 = ['user-definedlabeln'] #target labels\n",
        "\n",
        "ic2.remove('user-definedlabeln')\n",
        "oc2 = ['user-definedlabeln'] #target labels\n",
        "\n",
        "ic2.remove('user-definedlabeln')\n",
        "oc2 = ['user-definedlabeln'] #target labels\n",
        "\n",
        "ic3.remove('user-definedlabeln')\n",
        "oc3 = ['user-definedlabeln'] #target labels\n",
        "\n",
        "ic4.remove('user-definedlabeln')\n",
        "oc4 = ['user-definedlabeln'] #target labels\n",
        "\n",
        "ic4.remove('user-definedlabeln')\n",
        "oc4 = ['user-definedlabeln'] #target labels\n",
        "\n",
        "ic5.remove('user-definedlabeln')\n",
        "oc5 = ['user-definedlabeln'] #target labels\n",
        "\n",
        "ic6.remove('user-definedlabeln')\n",
        "oc6 = ['user-definedlabeln'] #target labels\n",
        "\n",
        "ic7.remove('user-definedlabeln')\n",
        "oc7 = ['user-definedlabeln'] #target labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jqu__XVzJENk"
      },
      "outputs": [],
      "source": [
        "X_train1 = train_df1[ic1]\n",
        "y_train1 = train_df1[oc1]\n",
        "X_test1 = test_df1[ic1]\n",
        "y_test1 = test_df1[oc1]\n",
        "\n",
        "X_train2 = train_df2[ic2]\n",
        "y_train2 = train_df2[oc2]\n",
        "X_test2 = test_df2[ic2]\n",
        "y_test2 = test_df2[oc2]\n",
        "\n",
        "X_train3 = train_df3[ic3]\n",
        "y_train3 = train_df3[oc3]\n",
        "X_test3 = test_df3[ic3]\n",
        "y_test3 = test_df3[oc3]\n",
        "\n",
        "X_train4 = train_df4[ic4]\n",
        "y_train4 = train_df4[oc4]\n",
        "X_test4 = test_df4[ic4]\n",
        "y_test4 = test_df4[oc4]\n",
        "\n",
        "X_train5 = train_df5[ic5]\n",
        "y_train5 = train_df5[oc5]\n",
        "X_test5 = test_df5[ic5]\n",
        "y_test5 = test_df5[oc5]\n",
        "\n",
        "X_train6 = train_df6[ic6]\n",
        "y_train6 = train_df6[oc6]\n",
        "X_test6 = test_df6[ic6]\n",
        "y_test6 = test_df6[oc6]\n",
        "\n",
        "X_train7 = train_df7[ic7]\n",
        "y_train7 = train_df7[oc7]\n",
        "X_test7 = test_df7[ic7]\n",
        "y_test7 = test_df7[oc7]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYx6PHuc8h7w"
      },
      "outputs": [],
      "source": [
        "ss = StandardScaler()\n",
        "\n",
        "X_trs1 = ss.fit_transform(X_train1)\n",
        "X_tes1 = ss.fit_transform(X_test1)\n",
        "\n",
        "X_trs2 = ss.fit_transform(X_train2)\n",
        "X_tes2 = ss.fit_transform(X_test2)\n",
        "\n",
        "X_trs3 = ss.fit_transform(X_train3)\n",
        "X_tes3 = ss.fit_transform(X_test3)\n",
        "\n",
        "X_trs4 = ss.fit_transform(X_train4)\n",
        "X_tes4 = ss.fit_transform(X_test4)\n",
        "\n",
        "X_trs5 = ss.fit_transform(X_train5)\n",
        "X_tes5 = ss.fit_transform(X_test5)\n",
        "\n",
        "X_trs6 = ss.fit_transform(X_train6)\n",
        "X_tes6 = ss.fit_transform(X_test6)\n",
        "\n",
        "X_trs7 = ss.fit_transform(X_train7)\n",
        "X_tes7 = ss.fit_transform(X_test7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP3jlWXSJSxC"
      },
      "source": [
        "**TRAINING THE INDEPENDENT CLASSIFIERS WITH OPTIMIZED PARAMETERS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzEAWFRUOozR"
      },
      "source": [
        "1.Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "svm = SVC(kernel='rbf', C=7, degree = 1, gamma= 'scale', probability = True)\n",
        "svm.fit(X_trs1,y_train1)\n",
        "print(f'The accuracy score of the model is {svm.score(X_tes1,y_test1.values.ravel()):.5f}')"
      ],
      "metadata": {
        "id": "Erq9LFmyCrNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxIJPMZKOtpK"
      },
      "source": [
        "2.k Nearest Neighbour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6WelVq2Tjpq"
      },
      "outputs": [],
      "source": [
        "KNN = KNeighborsClassifier(n_neighbors=120,\n",
        "                           weights='distance',\n",
        "                           metric='manhattan')\n",
        "KNN.fit(X_trs2,y_trs2)\n",
        "print(f'The accuracy score of the model is {KNN.score(X_tes2,y_test2.values.ravel()):.5f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNi69yU7OyxA"
      },
      "source": [
        "3.Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rfc = RandomForestClassifier(n_estimators=200,\n",
        "        criterion='entropy',\n",
        "        min_samples_split= 4,\n",
        "        min_samples_leaf= 4,\n",
        "        max_depth=10,\n",
        "        max_features='log2')\n",
        "rfc.fit(X_trs3, y_train3)\n",
        "print(f'The accuracy score of the model is {rfc.score(X_tes3,y_test3.values.ravel()):.5f}')"
      ],
      "metadata": {
        "id": "dlDZRFPlDQii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Decision Tree Classifier"
      ],
      "metadata": {
        "id": "WgwdGgjwkG0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dtc = tree.DecisionTreeClassifier()\n",
        "dtc.fit(X_trs4, y_train4)\n",
        "print(f'The accuracy score of the model is {dtc.score(X_tes4,y_test4.values.ravel()):.5f}')"
      ],
      "metadata": {
        "id": "MYGITP6ZL9Pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. ADABoost  Classifier"
      ],
      "metadata": {
        "id": "DxbwF_xLfEg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ada = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
        "ada.fit(X_trs6, y_train6)\n",
        "print(f'The accuracy score of the model is {ada.score(X_tes6,y_test6.values.ravel()):.5f}')"
      ],
      "metadata": {
        "id": "Osuf5FbtNCWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HM6D2gHO54n"
      },
      "source": [
        " 6.XGB Classifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xg1 = XGBClassifier(max_depth= 8,\n",
        "    learning_rate= 0.01998551759493316,\n",
        "    n_estimators= 56,\n",
        "    min_child_weight= 1,\n",
        "    gamma= 0.000187637803120415,\n",
        "    subsample= 0.8190504286066517,\n",
        "    colsample_bytree= 0.6611955891759931,\n",
        "    reg_alpha= 0.05945438336887176,\n",
        "    reg_lambda= 2.724508967377622e-05)\n",
        "xg1.fit(X_trs7, y_train7)\n",
        "print(f'The accuracy score of the model is {xg1.score(X_tes7,y_test7.values.ravel()):.5f}')"
      ],
      "metadata": {
        "id": "H62fOSr9Dxwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzQlH5t3O2NX"
      },
      "source": [
        "7.CatBoost Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwAT4l_pTvcp"
      },
      "outputs": [],
      "source": [
        "cat = catboost.CatBoostClassifier(iterations= 475,\n",
        "    learning_rate= 0.027583475549166746,\n",
        "    depth=4,\n",
        "    l2_leaf_reg= 1.0551779964424746e-05,\n",
        "    bootstrap_type='Bayesian',\n",
        "    random_strength= 2.0931628460945333e-07,\n",
        "    bagging_temperature= 0.923385947687978,\n",
        "    od_type='Iter',\n",
        "    od_wait= 26)\n",
        "cat.fit(X_trs5, y_train5)\n",
        "print(f'The accuracy score of the model is {cat.score(X_tes5,y_test5.values.ravel()):.5f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCPtmictMoue"
      },
      "source": [
        "**HETEROGENOUS SERIAL ENSEMBLE MODEL DEVELOPMENT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH1wo-VBMwYF"
      },
      "source": [
        "Calculating Pairwise Disagreement measure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qVNWcEYXO7B"
      },
      "outputs": [],
      "source": [
        "# Define the trained models\n",
        "models_1 = [svm,KNN,rfc,dtc,cat,ada,xg1]\n",
        "\n",
        "# Define the prediction function for a single model\n",
        "def predict_single_model(X, model):\n",
        "    y_pred = model.predict(X)\n",
        "    return y_pred\n",
        "\n",
        "# Define the disagreement function between two models\n",
        "def disagreement(y_pred1, y_pred2):\n",
        "    return np.sum(y_pred1 != y_pred2)\n",
        "\n",
        "# Compute the pairwise disagreements between all models\n",
        "disagreements = np.zeros((len(models), len(models)))\n",
        "for i in range(len(models)):\n",
        "    for j in range(i+1, len(models)):\n",
        "        y_pred1 = predict_single_model(X_tes1, models[i])\n",
        "        y_pred2 = predict_single_model(X_tes1, models[j])\n",
        "        disagreements[i][j] = disagreement(y_pred1, y_pred2)\n",
        "        disagreements[j][i] = disagreements[i][j]\n",
        "\n",
        "# Compute the disagreement scores for each model\n",
        "disagreement_scores = np.sum(disagreements, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXqdg7F9TidS"
      },
      "outputs": [],
      "source": [
        "model_names = [ 'svm','KNN','rfc','dtc','cat','ada','xg']\n",
        "\n",
        "# Plot the heatmap\n",
        "sns.set()\n",
        "fig, ax = plot.subplots(figsize=(10,8))\n",
        "sns.heatmap(disagreements, cmap=\"coolwarm\", annot=True, linewidths=0.5, ax=ax)\n",
        "\n",
        "# Replace the tick labels with the model names\n",
        "ax.set_xticklabels(model_names)\n",
        "ax.set_yticklabels(model_names_1)\n",
        "\n",
        "# Add labels and title\n",
        "ax.set_xlabel('Models')\n",
        "ax.set_ylabel('Models')\n",
        "ax.set_title('Pairwise Disagreement Heatmap')\n",
        "plot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7KWNvYqOIUi"
      },
      "source": [
        "Development of Sub Ensemble 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02QJeQT4Y23o"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "est_rfc = rfc\n",
        "#score_kNN=est_XB.fit(X_train,y_train).score(X_test,y_test)\n",
        "\n",
        "est_knn = KNN\n",
        "#score_knn=est_knn.fit(X_train,y_train).score(X_test,y_test)\n",
        "\n",
        "est_Ensemble1 = VotingClassifier(estimators=[('RFC', est_rfc), ('KNN', est_knn)],\n",
        "                        voting='soft',\n",
        "                        weights=[1, 1])\n",
        "est_Ensemble1.fit(X_trs1,y_train1)\n",
        "\n",
        "print(f'The accuracy score for the testing dataset is {est_Ensemble1.score(X_tes1, y_test1):.5f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating Pairwise Disagreement measure (Sub ensemble 1 + other classifiers)"
      ],
      "metadata": {
        "id": "f0t2VtuznqXT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVGEhW-OYocr"
      },
      "outputs": [],
      "source": [
        "# Define the trained models\n",
        "models_2 = [svm,est_Ensemble1,dtc,cat,ada,xg1]\n",
        "\n",
        "# Define the prediction function for a single model\n",
        "def predict_single_model(X, model):\n",
        "    y_pred = model.predict(X)\n",
        "    return y_pred\n",
        "\n",
        "# Define the disagreement function between two models\n",
        "def disagreement1(y_pred1, y_pred2):\n",
        "    return np.sum(y_pred1 != y_pred2)\n",
        "\n",
        "# Compute the pairwise disagreements between all models\n",
        "disagreements1 = np.zeros((len(models2), len(models2)))\n",
        "for i in range(len(models2)):\n",
        "    for j in range(i+1, len(models2)):\n",
        "        y_pred1 = predict_single_model(X_trs1, models1[i])\n",
        "        y_pred2 = predict_single_model(X_trs1, models1[j])\n",
        "        disagreements1[i][j] = disagreement1(y_pred1, y_pred2)\n",
        "        disagreements1[j][i] = disagreements1[i][j]\n",
        "\n",
        "# Compute the disagreement scores for each model\n",
        "disagreement_scores = np.sum(disagreements1, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqCpixKzZJG0"
      },
      "outputs": [],
      "source": [
        "model_names = ['SVM','En1','DTC','Cat', 'ADA','XG']\n",
        "\n",
        "# Plot the heatmap\n",
        "sns.set()\n",
        "fig, ax = plot.subplots(figsize=(10,8))\n",
        "sns.heatmap(disagreements, cmap=\"coolwarm\", annot=True, linewidths=0.5, ax=ax)\n",
        "\n",
        "# Replace the tick labels with the model names\n",
        "ax.set_xticklabels(model_names)\n",
        "ax.set_yticklabels(model_names_2)\n",
        "\n",
        "# Add labels and title\n",
        "ax.set_xlabel('Models')\n",
        "ax.set_ylabel('Models')\n",
        "ax.set_title('Pairwise Disagreement Heatmap')\n",
        "plot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Development of sub Ensemble 2"
      ],
      "metadata": {
        "id": "ojw-HMcj0syI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EokkSorFaTGG"
      },
      "outputs": [],
      "source": [
        "est_svm = svm\n",
        "#score_kNN=est_XB.fit(X_train,y_train).score(X_test,y_test)\n",
        "\n",
        "\n",
        "\n",
        "est_Ensemble2 = VotingClassifier(estimators=[('En1', est_Ensemble1), ('SVM', est_svm)],\n",
        "                        voting='soft',\n",
        "                        weights=[1, 1])\n",
        "est_Ensemble2.fit(X_trs1,y_train1)\n",
        "print(f'The accuracy score for the testing dataset is {est_Ensemble2.score(X_tes1, y_test1):.5f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating Pairwise Disagreement measure (Sub ensemble 2 + other classifiers)"
      ],
      "metadata": {
        "id": "qcAyRMREn_Ko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the trained models\n",
        "models1 = [ est_Ensemble2,dtc,cat,ada,xg1]\n",
        "\n",
        "# Define the prediction function for a single model\n",
        "def predict_single_model(X, model):\n",
        "    y_pred = model.predict(X)\n",
        "    return y_pred\n",
        "\n",
        "# Define the disagreement function between two models\n",
        "def disagreement1(y_pred1, y_pred2):\n",
        "    return np.sum(y_pred1 != y_pred2)\n",
        "\n",
        "# Compute the pairwise disagreements between all models\n",
        "disagreements1 = np.zeros((len(models1), len(models1)))\n",
        "for i in range(len(models1)):\n",
        "    for j in range(i+1, len(models1)):\n",
        "        y_pred1 = predict_single_model(X_trs1, models1[i])\n",
        "        y_pred2 = predict_single_model(X_trs1, models1[j])\n",
        "        disagreements1[i][j] = disagreement1(y_pred1, y_pred2)\n",
        "        disagreements1[j][i] = disagreements1[i][j]\n",
        "\n",
        "# Compute the disagreement scores for each model\n",
        "disagreement_scores = np.sum(disagreements1, axis=1)"
      ],
      "metadata": {
        "id": "R9xXI2pkmbcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_names = ['En2','dtc','cat', 'ada','xg']\n",
        "\n",
        "# Plot the heatmap\n",
        "sns.set()\n",
        "fig, ax = plot.subplots(figsize=(10,8))\n",
        "sns.heatmap(disagreements, cmap=\"coolwarm\", annot=True, linewidths=0.5, ax=ax)\n",
        "\n",
        "# Replace the tick labels with the model names\n",
        "ax.set_xticklabels(model_names)\n",
        "ax.set_yticklabels(model_names)\n",
        "\n",
        "# Add labels and title\n",
        "ax.set_xlabel('Models')\n",
        "ax.set_ylabel('Models')\n",
        "ax.set_title('Pairwise Disagreement Heatmap')\n",
        "plot.show()\n"
      ],
      "metadata": {
        "id": "vuYIWcb7miTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Development of the Sub Ensemble 3"
      ],
      "metadata": {
        "id": "p1X_T0K_07dB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxEovhIma27L"
      },
      "outputs": [],
      "source": [
        "est_xg = xg1\n",
        "\n",
        "est_Ensemble3 = VotingClassifier(estimators=[('En2  ', est_Ensemble2), ('XGBoost', est_xg)],\n",
        "                        voting='soft',\n",
        "                        weights=[1, 1])\n",
        "est_Ensemble3.fit(X_trs1,y_train1)\n",
        "\n",
        "print(f'The accuracy score for the testing dataset is {est_Ensemble3.score(X_tes1, y_test1):.5f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating Pairwise Disagreement measure (Sub Ensemble 3 + other classifiers)"
      ],
      "metadata": {
        "id": "nbSpT6b0o1tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the trained models\n",
        "models1 = [ est_Ensemble3,dtc,cat,ada]\n",
        "\n",
        "# Define the prediction function for a single model\n",
        "def predict_single_model(X, model):\n",
        "    y_pred = model.predict(X)\n",
        "    return y_pred\n",
        "\n",
        "# Define the disagreement function between two models\n",
        "def disagreement1(y_pred1, y_pred2):\n",
        "    return np.sum(y_pred1 != y_pred2)\n",
        "\n",
        "# Compute the pairwise disagreements between all models\n",
        "disagreements1 = np.zeros((len(models1), len(models1)))\n",
        "for i in range(len(models1)):\n",
        "    for j in range(i+1, len(models1)):\n",
        "        y_pred1 = predict_single_model(X_trs1, models1[i])\n",
        "        y_pred2 = predict_single_model(X_trs1, models1[j])\n",
        "        disagreements1[i][j] = disagreement1(y_pred1, y_pred2)\n",
        "        disagreements1[j][i] = disagreements1[i][j]\n",
        "\n",
        "# Compute the disagreement scores for each model\n",
        "disagreement_scores = np.sum(disagreements1, axis=1)"
      ],
      "metadata": {
        "id": "ABt2aWgVoJfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_names = ['En3','dtc','cat', 'ada']\n",
        "\n",
        "# Plot the heatmap\n",
        "sns.set()\n",
        "fig, ax = plot.subplots(figsize=(10,8))\n",
        "sns.heatmap(disagreements, cmap=\"coolwarm\", annot=True, linewidths=0.5, ax=ax)\n",
        "\n",
        "# Replace the tick labels with the model names\n",
        "ax.set_xticklabels(model_names)\n",
        "ax.set_yticklabels(model_names)\n",
        "\n",
        "# Add labels and title\n",
        "ax.set_xlabel('Models')\n",
        "ax.set_ylabel('Models')\n",
        "ax.set_title('Pairwise Disagreement Heatmap')\n",
        "plot.show()\n"
      ],
      "metadata": {
        "id": "9H7izYsYojIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Development of Sub Ensemble 4"
      ],
      "metadata": {
        "id": "jDJs1bwf1MwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "est_cat = cat\n",
        "\n",
        "est_Ensemble4 = VotingClassifier(estimators=[('En3', est_Ensemble3), ('CAT', est_cat)],\n",
        "                        voting='soft',\n",
        "                        weights=[1, 1])\n",
        "est_Ensemble4.fit(X_trs1,y_train1)\n",
        "\n",
        "print(f'The accuracy score for the testing dataset is {est_Ensemble4.score(X_tes1, y_test1):.5f}')"
      ],
      "metadata": {
        "id": "wnGeZTdLnR2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CONFUSION** **MATRIX**"
      ],
      "metadata": {
        "id": "9WPnitXrmD7q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sub - Ensembles"
      ],
      "metadata": {
        "id": "a08JO-RCp8Q4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred1 = est_Ensemble1.predict(X_tes1)\n",
        "confusion_matrix(y_test1, y_pred1)"
      ],
      "metadata": {
        "id": "Vwl9p2zqJzGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_e2 = est_Ensemble2.predict(X_tes1)\n",
        "confusion_matrix(y_test1, y_pred_e2)"
      ],
      "metadata": {
        "id": "k-crk4T4rewA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_e3 = est_Ensemble3.predict(X_tes1)\n",
        "confusion_matrix(y_test1, y_pred_e3)"
      ],
      "metadata": {
        "id": "ToKrdRzIpxzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_e4 = est_Ensemble4.predict(X_tes1)\n",
        "confusion_matrix(y_test1, y_pred_e4)"
      ],
      "metadata": {
        "id": "7tXpKfd3p2A4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Independent Classifiers"
      ],
      "metadata": {
        "id": "oBbjKWMAqByM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_predsvm = svm.predict(X_tes1)\n",
        "confusion_matrix(y_test1, y_predsvm)"
      ],
      "metadata": {
        "id": "GVOKGk_zqEci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predKNN = KNN.predict(X_tes1)\n",
        "confusion_matrix(y_test1, y_predKNN)"
      ],
      "metadata": {
        "id": "N1dCj6fqqab4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predrfc = rfc.predict(X_tes1)\n",
        "confusion_matrix(y_test1, y_predrfc)"
      ],
      "metadata": {
        "id": "Ox20iN1AqtKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_preddtc = dtc.predict(X_tes1)\n",
        "confusion_matrix(y_test1, y_preddtc)"
      ],
      "metadata": {
        "id": "F5mApWhPqx8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predada = ada.predict(X_tes1)\n",
        "confusion_matrix(y_test1, y_predada)"
      ],
      "metadata": {
        "id": "xwHU50pPq29n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predxg1 =xg1.predict(X_tes1)\n",
        "confusion_matrix(y_test1, y_predxg1)"
      ],
      "metadata": {
        "id": "gnR5JwEGq89F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predcat =xg1.predict(X_tes1)\n",
        "confusion_matrix(y_test1, y_predcat)"
      ],
      "metadata": {
        "id": "CQ1CWF6RrZUZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSmjzJ67O/9GKcmBtpGuFW"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}